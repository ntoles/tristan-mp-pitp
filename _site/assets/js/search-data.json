{"0": {
    "doc": "Code Features",
    "title": "Overview",
    "content": "Tristan-mp is implemented in Fortran 95, and is designed in a modular way, so that new relevant features can be added to the code with relative ease. In its present version, and without any alterations to the source code, Tristan-mp can simulate 2 different problems: UPDATE THIS: a shock simulation (with arbitrary plasma parameters defined in the input file) and, and a generic periodic run with two electron - ion counter-streaming beams (Weibel instability). The code numerically solves Maxwell’s equations, and the relativistic equations of motion for the particles. It is a massively parallel code, used and tested on high-performance computational clusters, and makes use of the Message Passing Interface (MPI) library, and the Hierarchical Data Format 5 (HDF5) libraries for outputs. There is an ongoing effort to vectorize the code using OpenMP. These are third party libraries that can be easily compiled for the specific system where the code is to be used (and are usually available in most of the high-performance clusters by default). On systems where there is not a specific implementation of the MPI library to be used, one possible implementation that can be downloaded and installed is the Open MPI library. In general, we find the none-free Intel MPI library to be slightly faster in practice. Below you can find a general description of the code structure, and how the code is divided among the several modules. Also, you can find on this page general performance information and general rules of thumb to use when setting up a simulation. For specific information on how to setup all the keywords in the input file visit the Input Structure page. Specific instructions on how to compile the code can be found here, and specific instructions on how to run the code are available here. ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Code-Features/#overview",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Code-Features/#overview"
  },"1": {
    "doc": "Code Features",
    "title": "General code structure",
    "content": "For Tristan-mp, each logical module corresponds to a Fortran 95 module, which corresponds to a specific implementation file. To run the code, you need to compile it, producing the correct binary files (two different binary files can be produced, for the 2D and 3D versions of the code). A root directory for the run must then be created, and the input file must be copied into it. UPDATE THIS The present version of the code, which is available from github (please contact the authors), consists of 20 F90 files, 1 .h file, 3 .c files, a MakefileD), and 2 sample input files. The actual number of files, however, might vary over time with different versions of the code being released. The distribution of the code among files can be roughly categorized in 7 sections: main files, main algorithms, outputs, initialization, communication, auxiliary modules, and user modules. We proceed to specify what can be found on each section. Main files . The main files of the code are the ones containing the calls to initialize all variables, and the main loop. These are tristan.F90 and tristanmainloop.F90. These are the top level files, which control the code flow; if you are actually seeking an in-depth knowledge on how the code works, you might want to start by looking at these two files, as they represent the highest level of abstraction. The file tristan.F90 is the main project file and actually just calls three functions whose names are self explanatory: initialize(), mainloop(), and finalize(). The mainloop() function is the core of Tristan-mp and is defined in tristanmainloop.F90; this function controls the flow of the code during the main part of the simulation. Here, functions are called to update the electromagnetic fields and to push particles around the simulation space, and to deposit charge on the grid. These functions actually implement the numerical solution to the physical equations solved in Tristan-mp and are defined in the next-subsection. Main algorithms . The files and modules of this abstract section deal mostly with the numerical implementation of the physical equations solved by the code. For a particle code such as Tristan-mp, this involves solving the electromagnetic field equations (Maxwell’s equations), and using the Electric and Magnetic fields calculated on the computational mesh to advance the particle’s velocities in time via the Lorentz force equation. Additional numerical steps that must be performed include interpolating the fields from the computational mesh to the particles positions, using a first order interpolating function, and depositing the current derived from the particles’ positions and velocities back to the same mesh. Also, a digital filtering function is used to suppress non-physical short-wavelength modes that result from the finite difference techniques employed. These modules thus contain the core of the code, and are defined in the files fields.F90 and particles.F90. The file fields.F90 contains functions that are directly related to the electromagnetic field solvers, including the digital filtering functions. The file particles.F90 includes all the functions that directly iterate over all the particle population, namely the velocity and position advance functions, and the functions used to deposit current on the mesh. Outputs . The outputs of the code are all in hdf5 files. All the output of the code is controlled and performed in the output.F90 file. Here you can find the functions that create the output files and process the data to be dumped to the files. Initialization . Initialization procedures are done by initializing the MPI communication library, reading the input for the run, and then requesting that each module reads the variables that are relevant to it. This behavior is implemented and controlled mainly in the initialize.F90 file, with corresponding implementations in each particular module. After reading the input variables, functions defined in each module are called in sequence for initialization. For example, the initialize_particles() function is called from initialize.F90; this function is actually defined and implemented in particles.F90. The purpose of this module is thus to centralize the initialization logic in one place, leaving the details of the actual module initialization to the modules themselves. Communication . Currently, the inter-process communication functions for each module are defined in the modules themselves (e.g. functions used to transfer parts of the computational mesh between processes are defined in fields.F90). This behavior however, might change in the near future, depending on how the code expands, and separate fieldcomm and particlecomm modules might be created. The purpose of the communications module file communications.F90 is to define general MPI-related functions that are useful for all the communication routines defined throughout the code. At this point this function just defines functions used to time the execution of the code, and the initialization of the random seed used in the random number generator of the code. The file mpidummy.F90 simply re-defines the MPI functions used in the code, and will only be relevant if the code is compiled in a system where no MPI library distribution is available. The use of this feature is discouraged, as it is a simple matter of installing a free distribution of the MPI libraries to have the full functionality of the code available. Auxiliary modules . OUT OF DATE, UPDATE . The auxiliary module files are the ones handling the auxiliary features of the code, some essential, others not. These include the files aux.F90, domain.F90, restart.F90, inputparser.F90, fparser.F90, system.F90, selectprt.F90, and splittestpart.F90. These two last files define independent programs used for the particle track diagnostics of the code. In the current version of the code, the file aux.F90 just defines the random number generator. The file domain.F90 defines functions that allow the computational domain to grow in time, which is used for efficiency in the shock simulations. The file inputparser.F90 links to par.c and par.h which are actually responsible for parsing the input file. The fparser.F90 module is generica mathematical function parser, that allows the user to specify arbitrary functions in the input file (which can then be used to setup the simulation). Finally, the restart.F90 file defines functions that allow the complete state of a simulation to be stored on a disk at a given point and restarted later, and the file system.F90 defines system specific auxiliary functions. User module . The user file, user_*.F90, such as user_shock.F90 or user_weibel.F90, is used for readily expanding the behavior of the code without having to alter any of the code-flow logic. The user defines routines for loading and injecting particles here, as well special boundary conditions on fields and particles (e.g. reflecting walls). The purpose of the user module is to make the expansion and generalization of the code more accessible, in particular when this does not involve modifying the underlying numerical methods used to solve the physical equations. Expansion of the code by adding other modules and modifying the core structure of the code might also be attempted, but will probably require more time, and a more complete understanding of the underlying structure of the entire code. ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Code-Features/#general-code-structure",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Code-Features/#general-code-structure"
  },"2": {
    "doc": "Code Features",
    "title": "Code Features",
    "content": " ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Code-Features/",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Code-Features/"
  },"3": {
    "doc": "Compiling Tristan",
    "title": "Compilation",
    "content": "There are three different steps involved in the compilation of Tristan-mp. The first two steps are the compilation of the MPI and HDF5 libraries if they are not already available on the system you are going to run. For most cases, there will be a preferred system-specific MPI implementation for most of the high-performance computational clusters where you should be running Tristan-mp. You should always use this MPI library implementation; just be aware that Fortran bindings to the library are necessary (they usually are included by default). Regarding the HDF5 libraries, when available they still might not have the required fortran bindings and/or parallel output bindings, which are a requirement of the code. As such you might have to compile your own version of the HDF5 library. The following two sections show you how to do this, but they are not intended as a substitute for the documentation of the libraries, and assume a basic knowledge of Unix systems. For more information read the documentation available with the MPI and HDF5 library distributions. ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/#compilation",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/#compilation"
  },"4": {
    "doc": "Compiling Tristan",
    "title": "MPI library",
    "content": "The preferred MPI distribution to use (other than any system specific distribution available), is the Open MPI library available for download here. After downloading the source files for the library (as a .tar.gz file or similar), you will need to copy this to the remote system where it is to be installed. To do that you will usually use a command such as scp, as in scp downloaded-file username@remotesystem: . You might be asked for your password to the remote system, and this should copy the file to your home directory there. Next step is to login into the system (usually via ssh). You will now have to untar / unzip the .tar.gz file, using a command such as tar -xzf filename or similar, and this will create the installation directory. Change directories to the installation directory. Now you will have to figure out the appropriate configuration options for the library. You will do this by typing ./configure --help. At the very least, you will have to change the final installation directory, and make sure that Fortran bindings are included in the distribution (they usually are by default). Also, you will have to configure the correct C and Fortran compilers for the library to use (such as gcc and gfortran, or icc and ifort). These should be the compilers that are recommended for the system you are using. After figuring out the configuration options, you will just have to run: ./configure (passing in the options), make and make install. This will produce the final library installation diretory with the .dylib (and / or .a files), and the executable files mpicc and mpif90, among others. These two will be necessary for the next step. ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/#mpi-library",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/#mpi-library"
  },"5": {
    "doc": "Compiling Tristan",
    "title": "HDF5 library",
    "content": "Installing the HDF5 libraries is similar to the process described above for the MPI libraries. You first download the source files for the library here. After that, you copy to the remote system, and untar / unzip as described above. Once logged in the system, the first step is to figure out if the mpicc and mpif90 compiler wrappers are available in the system. When you type which mpicc and which mpif90, you should see the complete path to these executables; the path should correspond to the appropriate MPI library that you will be using (either a system one, or the one you installed). For this to work you might have to add the path for mpicc and mpif90 to your path variable, if you did not do so already. You do this by typing . setenv PATH ${PATH}:path-to-the-mpi-distribution-bin-folder . (for csh, tcsh, or similar), or . export PATH=${PATH}:path-to-the-mpi-distribution-bin-folder . (for sh, bash, ksh, or similar). Next, you will have to determine the correct options to pass to ./configure, as in the installation for the MPI library above. At the very least, you will configure a correct destination folder (a sub-directory on your home folder), and turn on Fortran bindings and parallel bindings. Also, the compilers to use will be mpicc and mpif90 for C and Fortran, respectively. After this, you will run ./configure (passing in the options), make and make install. This will produce the final library installation directory with the .dylib (and / or .a files), and the executable file h5pfc, among others. This file will be necessary for the next step. Example: ./configure --help . this lists the options . export CC=mpicc export CXX=mpicc export FC=mpif90 configure --prefix=/home/username/bin/hdf5 --enable-parallel --enable-fortran make install . ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/#hdf5-library",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/#hdf5-library"
  },"6": {
    "doc": "Compiling Tristan",
    "title": "Tristan-mp compilation",
    "content": "After downloading the code source, and copying the source to your cluster, you can compile the code to produce binaries for 2D or 3D version. As a first step, you have to make sure that the h5pfc tool is available in the HDF5 library distribution you are using. This file should be in the bin directory of the library distribution. If it is not, you will have to recompile the HDF5 libraries following the instructions above. Once you know where this executable is located, add it to your path, so that it is accessible by the Tristan-mp Makefile (and Makefile3D). You usually do that by executing . setenv PATH ${PATH}:path-to-the-library-bin-folder . (in c type shells - csh, tcsh and such), or . export PATH=${PATH}:path-to-the-library-bin-folder . (in sh, bash, ksh, or other similar shells). It is a good idea to add these commands to your .bashrc or .cshrc file. If when you type which h5pfc you get the path to the binary file, you are set to go. Just change directory to the Tristan-mp distribution’s source folder and type make to get the 2D version of the code). THIS IS IMPORTANT! . The code is organized in a modular way, so that in most cases the user does not need to edit any of the source files except for the user_*.F90 file. Two examples, user_weibel.F90 and user_shock.F90 are provided. Corresponding input files are also in the user directory. The choice of which user file will be compiled is made by modifying the USER variable in the source/Makefile or Makefile3D. The standard workflow for a new problem definition is to copy one of the example user_* files to a new user_myrun.F90 file (where myrun is something descriptive) and editing this user file. There are routines for initializing particles, injecting particles during the run, and special particle and field boundary conditions which are listed in the user_*.F90 file. When ready, there is no reason to edit the Makefile, instead run . make clean make USER_FILE=user_myrun . and set USER=user_myrun, or whatever is the name of your user file omitting .F90 . IT IS VERY EASY TO FORGET THE USER_FILE flag, and the code will compile the old user module. Not very useful. The name of the user file is printed in the beginning of the out file when the code runs. So, check that if the result does not make sense. Note that you might also have to alter some of the pre-processor and/or processor options under the keyword CUSTOM in the Makefile (Makefile and Makefile3D), which reads something like CUSTOM= -DMPI -DHDF5 -DserIO -fpp -DtwoD. This is just defining keywords to be passed to the pre-processor, and since the way to do this is compiler dependent you might have to change it. The code in the current version of the repository will compile by default using gcc and gfortran. Some notes: When working with both 2D and 3D version, make sure to run “make clean” or “make -f Makefile3D clean” before switching to compilation in different number of dimensions. Some of the libraries have the same names, but are compiled with different flags. Not doing this will in general lead to a compilation failure. ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/#tristan-mp-compilation",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/#tristan-mp-compilation"
  },"7": {
    "doc": "Compiling Tristan",
    "title": "Compiling Tristan",
    "content": "THIS SECTION IS OLD AND DEPRECATED. PLEASE SEE THIS INSTEAD . ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Compilation/"
  },"8": {
    "doc": "Compiling Tristan-MP",
    "title": "Downloading Tristan-MP from Github",
    "content": "Inside of perseus, cd to your tigress directory . cd ~/tig . Download tristan-mp from github.com to perseus by cloning the directory. git clone https://github.com/PrincetonUniversity/tristan-mp-pu.git . Go into the tristan-mp-pu folder . cd tristan-mp-pu . ",
    "url": "http://localhost:4000/GettingStarted/Downloading-and-Compiling-Tristan/#downloading-tristan-mp-from-github",
    "relUrl": "/GettingStarted/Downloading-and-Compiling-Tristan/#downloading-tristan-mp-from-github"
  },"9": {
    "doc": "Compiling Tristan-MP",
    "title": "Creating your own Tristan-MP branch and user files",
    "content": "Create your own branch of Tristan-MP where you can edit your user-files . git checkout -b YOUR_NAME . The structure of Tristan is such that you should not have to edit the main code to change the set-up of the files (more details here). Instead, the simulation set-up is changed using user_files that define important functions in the code. You should use your own user files. Make a new directory where you will save your user files . mkdir user_NAME . Copy the code you want to work with. We’ll set-up Tristan-mp to run a shock problem. Each problem has its own user file and input file. cp user_pu/user_shock_pu.F90 user_NAME/user_myshock.F90 cp user_pu/input.shock_pu user_NAME/input.myshock . There is a line in user_myshock.f90 that writes the userfile name in the output while tristan-mp is running. This is a very convenient feature that lets you know what you compiled your run with, but must be updated for every new user file. Find the line. The easiest way to do this with nano is nano user_NAME/user_myshock.F90. Inside of nano, you can search for a string with ctrl+W then type ‘user_shock_pu’. Change the line from . if(rank.eq.0) print *, \"Using user file user_shock_pu.F90\" . to . if(rank.eq.0) print *, \"Using user file user_NAME/user_myshock.F90\" . Save and close the file in nano by pressing ctrl+x and then y and enter to the prompts. ",
    "url": "http://localhost:4000/GettingStarted/Downloading-and-Compiling-Tristan/#creating-your-own-tristan-mp-branch-and-user-files",
    "relUrl": "/GettingStarted/Downloading-and-Compiling-Tristan/#creating-your-own-tristan-mp-branch-and-user-files"
  },"10": {
    "doc": "Compiling Tristan-MP",
    "title": "Compiling",
    "content": "Compilation in Tristan-MP is handled with make and a Makefile. Make is a very full-featured program with many options. Luckily, you shouldn’t have to mess too much with the Makefile at first. cd into the ~/tig/tristan-mp-pu folder. We will want to compile tristan-mp, but before doing so it is good practice to first clean the directory in case you changed some files: type . make clean . While not always necessary, I think it is good habit to always run make clean before running make. Now compile tristan-mp pointing it towards your user file. make USER_FILE=user_NAME/user_myshock . The executable “tristan-mp2d” should appear in the newly created ~tig/tristan-mp/exec directory, check that it is there. ls exec . If instead you get a warning stating something along the lines of . h5pcc -O3 -c code/system.c -o obj/system.o make: h5pcc: Command not found make: *** [obj/system.o] Error 127 . That means you have not loaded the proper modules to compile the code. Please ensure you are logged into perseus by typing bash hostname. If you are logged into perseus, check that you have loaded all the neccessary modules by typing bash module list you should find the following modules intel, intel-mpi, hdf5/intel-16.0/intel-mpi/1.8.16 See more here. Remember to run make clean before trying to recompile. ",
    "url": "http://localhost:4000/GettingStarted/Downloading-and-Compiling-Tristan/#compiling",
    "relUrl": "/GettingStarted/Downloading-and-Compiling-Tristan/#compiling"
  },"11": {
    "doc": "Compiling Tristan-MP",
    "title": "Compiling Tristan-MP",
    "content": "This section assumes you have already logged into perseus and followed all the instructions to ensure you have loaded all the modules required to compile tristan-MP . ",
    "url": "http://localhost:4000/GettingStarted/Downloading-and-Compiling-Tristan/",
    "relUrl": "/GettingStarted/Downloading-and-Compiling-Tristan/"
  },"12": {
    "doc": "Input Files",
    "title": "Input file structure",
    "content": "This page describes in detail what each keyword in the input file does, the possible values and the default values assumed by the code. Below there is a sample input file for reference purposes. This input specifies a 2D simulation with 800 cells in the x direction, and 256 cells in the y direction. The simulation domain is decomposed in 16 blocks (CPU’s / cores), along the y direction. The maximum number of particles supported is 6 x 10^7, and the physical problem being run is a Weibel instability simulation in external magnetic field. Strength of external field is set by parameter sigma_ext. The velocity of light is 0.45 (simulation units), and the magnetization of the flow (sigma) is 0. The electron collisionless skin depth ( c / omega_pe) is 8, and the bulk flow velocity of the particles has a gamma of 100. Also, there are 10 simulation particles per cell in the box initially, the electron mass is set to 1, and the ion mass is set to 1. For a detailed description of each keyword in the input file check the Detailed Keyword Description section. The input file is structured around named sections, which should stand on its own line, have a unique name, and be surrounded by &lt;&gt; (as in ). Any line starting with a # is a comment and is ignored by the input parser (any information after the # in a given line is ignored). Furthermore, any variables which are specified but not explicitly read in the code are just ignored, and if some keyword is omitted, a default value is usually assumed (which might lead to unexpected results). There are currently 10 sections that are currently being parsed: , , , , , , , , , , and . Each section is concerned with a specific part of the algorithm, or with the control of some specific behavior of the code. ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Input-Files/#input-file-structure",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Input-Files/#input-file-structure"
  },"13": {
    "doc": "Input Files",
    "title": "Sample Input File",
    "content": "# # &lt;node_configuration&gt; sizey = 16# number of cpus in the y direction &lt;time&gt; last = 2000# last timestep c= .45# velocity of light in comp. units (this defines the timestep) timespan= 86400# time, in seconds, available to run the problem &lt;grid&gt; mx0 = 800 # number of actual grid points in the x direction my0 = 256# number of actual grid points in the y direction mz0 = 256 # ... (ignored for 2D simulations) &lt;algorithm&gt; conserv = 1 # charge-conservative current deposition -- the only available option highorder= 0 # 0 -- 2nd order FDTD field integrateion; 1 -- 4th order; # don't use 1 for non-relativistic flows Corr= 1.025 # correction for the speed of light ntimes= 4# number of passes for smoothing filter (current) cleanfld= 0# number of passes for smoothing filter (fields). don't use. cleanint= 10# interval for field cleaning; don't use. cooling= 0# cool particles? ; not implemented acool= 10.# cooling parameter for particles splitparts = 0# split particles to improve statistics? &lt;restart&gt; irestart= 0# 1 to restart the simulation from saved restart/*.d files. intrestart= 1000# how often to save restart files. They overwrite previous *d files. laprestart= 0# if different from 0, restart from a named restart file, saved at timestep laprestart namedrestartint = 1000000 # interval for saving named restart files, like restart.lap01234.d &lt;output&gt; interval = 20# plot interval torqint= 2000000# interval for outputs at different resolution (currently broken) pltstart= 0# starting iteration for first plot istep= 1# downsampling factor for grid output istep1= 4# downsampling factor for grid output every torqint steps stride= 10# particle stride for particle output writetestpart= 0# write test particles? selectprt= 0# re-trace the same selected particles? &lt;boundaries&gt; periodicx= 1# periodic boundaries in the x direction? Choose 0 for radiative boundaries. periodicy= 1# periodic boundaries in the y direction? periodicz= 1# periodic boundaries in the z direction? &lt;domain&gt; enlarge= 0# if 1, enlarge box in the x direction if injector is close to right wall? movwin = 0# if 1, use moving window shiftinterval = 20# how often to apply moving window (in steps) shiftstart = 1000# at what step to start shifting moving window movwingam = 5. # gamma factor of moving window. If &gt; 10000, it moves at c. # if &lt; 1, it is interpreted as v/c. &lt;fields&gt; btheta= 85# bfield angle bphi=0 -&gt; bz, bph=90 in x-y plane, bth=0-&gt; parallel bphi= 90# &lt;particles&gt; sigma= 0.# magnetization number (omega_c/omega_p)^2, including gamma0 maxptl0 = 6e7# max number of particles in the simulation ppc0 = 10# number of particles per cell delgam = 1.e-4# delta gamma (temperature control) me= 1.# electron mass mi= 1.# ion mass (actually mass to charge ratio) gamma0= 100.# flow drift gamma. If &lt; 1, interpreted as v/c. c_omp= 8# electron skin depth in cells &lt;problem&gt; caseinit = 1 #can be used to select subcases of the problem. Not used. #density_profile=0. #x,y,z dependent function distributing initial particle weight temperature_ratio = 1 # T_e/T_i external_fields = 1 # if nonzero, add external nonevolving fields to mover sigma_ext = 0.1 # strength of external magnetization,(omega_c/omega_p)^2, # including gamma0 user_part_bcs=1 # call particle_bc_user routine from user file, specify particle bcs like walls wall = 0 # use reflecting wall? Position is set in user file. wallgam = 0. # gamma of the moving reflecting wall. If &lt; 1, read as v/c. Set to 0 or 1 to not move the wall. Keyword Specification . mx0, my0, and mz0 . Actual number of grid points in x,y and z directions . MAXPTL0 .... ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Input-Files/#sample-input-file",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Input-Files/#sample-input-file"
  },"14": {
    "doc": "Input Files",
    "title": "Input Files",
    "content": "NEEDS UPDATING: VERY OUT OF DATE . When tristan-mp is run, it looks for a file named input inside of the current directory. The input file contains all of the . ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Input-Files/",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Input-Files/"
  },"15": {
    "doc": "Logging into Perseus",
    "title": "First time accessing Perseus and/or Tigressdata",
    "content": "The first time you try to access Perseus and/or Tigressdata from you local machine you must be on Princeton’s network or use a VPN connection. If you’re off-campus first you’ll need to install a VPN to connect to the cluster. VPN access to Princeton is well documented and you will find information about how to do it here.. Follow the instructions on how to install sonic wall connect and VPN into the local Princeton network. ",
    "url": "http://localhost:4000/GettingStarted/Logging-in-to-perseus/#first-time-accessing-perseus-andor-tigressdata",
    "relUrl": "/GettingStarted/Logging-in-to-perseus/#first-time-accessing-perseus-andor-tigressdata"
  },"16": {
    "doc": "Logging into Perseus",
    "title": "Perseus Information",
    "content": "Perseus is the supercomputer where we run most of our Tristan-MP development runs. Perseus is a 320 node Dell Beowulf cluster. All compute nodes in this cluster are connected via an Infiniband network designed for high speed and low latency to enable excellent performance for tightly coupled MPI codes. Each node has 28-core Broadwell processors and 128 GB of RAM. General Guidelines . The head node, perseus, should be used for interactive work only, such as compiling programs, and submitting jobs as described below. No jobs should be run on the head node, other than brief tests that last no more than a few minutes. Where practical, we ask that you entirely fill the nodes so that CPU core fragmentation is minimized. For this cluster, perseus, that means multiples of 28 cores. Please remember that these are shared resources for all users. ",
    "url": "http://localhost:4000/GettingStarted/Logging-in-to-perseus/#perseus-information",
    "relUrl": "/GettingStarted/Logging-in-to-perseus/#perseus-information"
  },"17": {
    "doc": "Logging into Perseus",
    "title": "Logging into Perseus.",
    "content": "Connect to perseus. Type in terminal: . ssh netID@perseus.princeton.edu . If you have not already done so, it is highly recommended that you copy and source Anatoly’s .bashrc file into your perseus account. cp ~anatoly/.bashrc ./ source ~/.bashrc . If instead, you prefer manually loading the modules necessary to compile Tristan-MP, type: . module load intel module load intel-mpi module load hdf5/intel-16.0/intel-mpi/1.8.16 . ",
    "url": "http://localhost:4000/GettingStarted/Logging-in-to-perseus/#logging-into-perseus",
    "relUrl": "/GettingStarted/Logging-in-to-perseus/#logging-into-perseus"
  },"18": {
    "doc": "Logging into Perseus",
    "title": "Princeton Clusters file system.",
    "content": "Right now you should be in your perseus home folder. While you may find it useful to have a few files here, most time you will save your data on a large filesystem that is backed up to tape &amp; accessible from all of the Princeton supercomputers, Tigress. Go to your tigress folder. This space is your main folder where we will keep all our simulations. cd /tigress/netID . You may find it useful to create a symlink in your home directory to your tigress folder. e.g. ln -sf /tigress/netID ~/tig . This enables you to cd to the directory by simply typing cd ~/tig. There also is a scratch directory which is not backed up to tape that you can use to save development runs. Make a symlink to the scratch disk. ln -sf /scratch/gpfs/netID ~/scratch . ",
    "url": "http://localhost:4000/GettingStarted/Logging-in-to-perseus/#princeton-clusters-file-system",
    "relUrl": "/GettingStarted/Logging-in-to-perseus/#princeton-clusters-file-system"
  },"19": {
    "doc": "Logging into Perseus",
    "title": "Logging into Perseus",
    "content": " ",
    "url": "http://localhost:4000/GettingStarted/Logging-in-to-perseus/",
    "relUrl": "/GettingStarted/Logging-in-to-perseus/"
  },"20": {
    "doc": "Logging into TigressData2",
    "title": "Tigressdata2",
    "content": "Tigressdata2 is a computer that has access to the files you created when running your simulations. It has a lot of RAM and is very convenient way to analyze your simulations without have to move the data anywhere. Most of the time, you’ll login to tigressdata2 via VNC, a way to have a remote desktop. ",
    "url": "http://localhost:4000/AnalyzingSims/Logging-in-to-tigressdata2/#tigressdata2",
    "relUrl": "/AnalyzingSims/Logging-in-to-tigressdata2/#tigressdata2"
  },"21": {
    "doc": "Logging into TigressData2",
    "title": "Starting a VNC server on tigressdata2",
    "content": "Open terminal on your local machine and login to tigressdata2 . ssh netID@tigressdata2.princeton.edu . Start a vncserver on tigressdata2. The geometry flag sets the size of the vnc window. vncserver -geometry 1680x1050 . The first time you start vncserver you will be prompted to create a password to access you vnc desktops. This can be anything you like. Should you choose to change it in the future simply delete the file ~/.vnc/passwd, and the next time to start a vnc session you will be prompted to recreate a password. You may choose to have a view-only password, where others can view your desktop, but keystrokes are disabled. You should get an output that looks like this: . Desktop 'TurboVNC: tigressdata2.princeton.edu:1 (NetID)' started on display tigressdata2.princeton.edu:1 Creating default startup script /home/NetID/.vnc/xstartup.turbovnc Starting applications specified in /home/NetID/.vnc/xstartup.turbovnc Log file is /home/NetID/.vnc/tigressdata2.princeton.edu:1.log . You will want to note the note the session number. In the above example it is :1. You can always check what vnc session(s) you have open with the vncserver -list command which gives an output like: . TurboVNC server sessions: X DISPLAY # PROCESS ID :1 18966 . ",
    "url": "http://localhost:4000/AnalyzingSims/Logging-in-to-tigressdata2/#starting-a-vnc-server-on-tigressdata2",
    "relUrl": "/AnalyzingSims/Logging-in-to-tigressdata2/#starting-a-vnc-server-on-tigressdata2"
  },"22": {
    "doc": "Logging into TigressData2",
    "title": "Connecting to Tigressdata2",
    "content": "First download and install TurboVNC on your local machine: turboVNC . From your local machine, you will use the TurboVNC viewer to connect, but first you need to set up an ssh tunnel. This is necessary because TurboVNC is not encrypted. The session number from earlier is ‘n’ here. e.g. on a Unix machine like MacOS or Linux type . ssh -A -L &lt;5900+n&gt;:localhost:&lt;5900+n&gt; &lt;user&gt;@tigressdata2.princeton.edu . Example for the above session (:1) . ssh -A -L 5901:localhost:5901 netID@tigressdata2.princeton.edu . You may see a message or dialog box similar to this: The server's host key is not cached in the registry. If you do, type or click yes. Enter the password associated with your netID. Now on your local computer, open the TurboVNC Viewer and in the VNC server box type: localhost:&lt;n&gt; and click connect. For example: localhost:1. Enter the password you set up when you created your vncserver session above. You should now be able to see your tigressdata2 desktop. ",
    "url": "http://localhost:4000/AnalyzingSims/Logging-in-to-tigressdata2/#connecting-to-tigressdata2",
    "relUrl": "/AnalyzingSims/Logging-in-to-tigressdata2/#connecting-to-tigressdata2"
  },"23": {
    "doc": "Logging into TigressData2",
    "title": "Closing a VNC Session",
    "content": "You can close the TurboVNC viewer, reconnect at a later time, and pick up right where you left off. Just follow the procedure for connecting to a remote VNC session above. If the tunnel is closed, the session will end, but your desktop will still be intact on tigressdata. Just reconnect. When you are finished with a VNC session entirely, you should end it. To end a vnc session ssh to tigressdata2 . If you haven’t already, load the turbovnc module . module load turbovnc . Check the number of your vnc session with vncserver -list and note the display # of the session you want to end . Do vncserver -kill :&lt;n&gt; where n is the number (with a colon). e.g . vncserver -kill :1 . ",
    "url": "http://localhost:4000/AnalyzingSims/Logging-in-to-tigressdata2/#closing-a-vnc-session",
    "relUrl": "/AnalyzingSims/Logging-in-to-tigressdata2/#closing-a-vnc-session"
  },"24": {
    "doc": "Logging into TigressData2",
    "title": "Enabling tigressdata2’s GPUs",
    "content": "To take full advantage of the local graphics hardware of tigressdata2 we use VirtualGL. Note this will only work within a VNC session. The instructions here are for a terminal open on your VNC desktop. Check to make sure you have the virtualgl module loaded, if not do . module load virtualgl . Start a program as you normally would but precede the command with vglrun. Some examples: . vglrun visit vglrun paraview vglrun /usr/licensed/matlab/bin/matlab . ",
    "url": "http://localhost:4000/AnalyzingSims/Logging-in-to-tigressdata2/#enabling-tigressdata2s-gpus",
    "relUrl": "/AnalyzingSims/Logging-in-to-tigressdata2/#enabling-tigressdata2s-gpus"
  },"25": {
    "doc": "Logging into TigressData2",
    "title": "Jupyter notebook through ssh",
    "content": "Most of the time the data is stored on the server, but it’s a lot more convenient and sometimes faster to analize the results from a personal laptop. For that you can start a jupyter session on a tigressdata2 machine and connect to it through ssh from you personal device and access all the jupyter capabilities from your local browser. For that you would first need a jupyter loaded on tigressdata2, which is contained in anaconda package, so run this in terminal . module load anaconda . Then you want to run the following command in any folder you want the jupyter root to be in . jupyter notebook --no-browser --port=8889 --ip=127.0.0.2 . Port number and ip could be anything; if a particular value doesn’t work, it may be used by some other process. You may also specify a bash function in your ~/.bashrc or ~/.bash_profile in the following way . jupyter_run() { jupyter notebook --no-browser --port=$1 --ip=127.0.0.2 } . which you later can use the following way $ jupyter_run 8889. Now you need to connect to this server from your local laptop through ssh. For that you can simply run the following command in your local terminal . ssh -N -L localhost:8889:127.0.0.2:8889 &lt;username&gt;@tigressdata2.princeton.edu . where &lt;username&gt; is your tigressdata2 username, and the port number 8889 should correspond to the one specified earlier. In the same fashion you can create an alias function in ~/.bashrc or ~/.bash_profile . ssh_jupyter() { ssh -N -L localhost:$1:127.0.0.2:$1 &lt;username&gt;@tigressdata2.princeton.edu } . and call it later as ssh_jupyter 8889. You can now access the jupyter notebook via localhost:8889 in your local browser. ",
    "url": "http://localhost:4000/AnalyzingSims/Logging-in-to-tigressdata2/#jupyter-notebook-through-ssh",
    "relUrl": "/AnalyzingSims/Logging-in-to-tigressdata2/#jupyter-notebook-through-ssh"
  },"26": {
    "doc": "Logging into TigressData2",
    "title": "Additional Information",
    "content": "See instructions here . ",
    "url": "http://localhost:4000/AnalyzingSims/Logging-in-to-tigressdata2/#additional-information",
    "relUrl": "/AnalyzingSims/Logging-in-to-tigressdata2/#additional-information"
  },"27": {
    "doc": "Logging into TigressData2",
    "title": "Logging into TigressData2",
    "content": " ",
    "url": "http://localhost:4000/AnalyzingSims/Logging-in-to-tigressdata2/",
    "relUrl": "/AnalyzingSims/Logging-in-to-tigressdata2/"
  },"28": {
    "doc": "Running Tristan-MP",
    "title": "Getting ready",
    "content": "Now, let’s run your first shock simulation! . cd back to your tigress folder . cd ~/tig . Generally, you’ll want to run each simulation in its own directory. Let’s make a directory for your first run, and cd into it . mkdir firstrun cd firstrun . Now, we need to add our input file and executable to the firstrun folder. cp ~/tig/tristan-mp-pu/exec/tristan-mp2d ./ cp ~/tig/tristan-mp-pu/user_NAME/input.myshock ./ . Check the contents of firstrun folder by typing ls. You should be able to confirm files were copied. ",
    "url": "http://localhost:4000/GettingStarted/Running-your-first-Tristan-MP-simulation/#getting-ready",
    "relUrl": "/GettingStarted/Running-your-first-Tristan-MP-simulation/#getting-ready"
  },"29": {
    "doc": "Running Tristan-MP",
    "title": "Editing the Input file",
    "content": "The parameters of the shock run e.g. size of the box, number of particles, etc. is set using a input file. The input file has different sections demarcated by a &lt;SECTION NAME&gt; Let’s go over each section of the input.myshock file. Follow along by typing nano input.shock. Node configuration . # # Tristan-mp input file # # &lt;node_configuration&gt; sizex = 16 # number of cpus in x direction sizey = 1 # number of cpus in the y direction . This section defines the number of MPI jobs AKA cores/CPUs used in each direction. In general, it’s best practice to completely fill nodes on your system. The total number of cores required is sizex*sizey. On perseus each node has 28 cores. Change sizex to 4 and sizey to 7. Later, when we submit the job we’ll need to tell the scheduler we’ll take 28 cores. Time . &lt;time&gt; last = 600000 # last timestep c = .45 # velocity of light in comp. units # this defines the timestep timespan = 60000000 # time, in seconds, available to run the problem, # to not save restart before the end . In general you shouldn’t need to change this section. last is the timestep where the program will terminate. c is the speed of light in the grid. In PIC simulations c is traditionally set to 0.45 which means a photon will propagate 0.45 cells in one time step. This value guarantees that the Courant condition is satisfied. I’m not sure that timespan does. Since we’re just running a short test run, let’s set last to 15000. Grid . &lt;grid&gt; mx0 = 10000 # number of actual grid points in the x direction my0 = 128 # number of actual grid points in the y direction mz0 = 1 # ... (ignored for 2D simulations) . The part of the input file that sets the size of the simulation domain in cells. Since we’re running the simulation for 15000 time steps, lets set mx0 to 10000. In production runs, you should take advantage of the expanding box, but for now, let’s keep things simple. Also sizey should be a factor of my0. Since we chose sizey=7 set my0=133 . Load balancing . &lt;dynamic_grid&gt; dynmx = 1 # 1 for dynamic allocation of mx dynmxstart = 1 # starting timestep dynmxinterval = 200 # update interval mxmin = 7 # minimum mx/cpu dynmy = 0 dynmystart = 1 dynmyinterval = 200 mymin = 7 . This part of the input file are related to load balancing the code. In general, you want a roughly equal number of particles on each code, so each core is working as hard as all the others. The simulation has to wait until all cores are done with the current lap before it can start the next lap. Initially each core takes a equal sized part of the grid, but if there are strong density fluctuations, some parts of the grid become more expensive than others. If you choose, tristan-MP can try to dynamically re-allocate the grid. The parameters dynmx and dynmy are set to 1, tristan will reallocate in the x and y direction respectively. dynm*start Is the starting lap to reallocate the grid, this should almost always be set to 1. dynm*interval chooses how many laps between load balancing. m*min is the minimum size the dynamic decomposition will allow. If you make it less than 7 you’ll spend too much time calculating ghost zones. Don’t change anything here. Field solver . &lt;algorithm&gt; highorder = 0 # 0 -- 2nd order FDTD field integration; 1 -- 4th order; # don't use 1 for non-relativistic flows Corr = 1.0 # correction for the speed of light ntimes = 32 # number of passes for smoothing filter (current) . These parts of the input change parts of the field solver. Setting highorder to 1 if particles are moving relativistic flows is useful because it helps with numerical Cherenkov. Corr is a special parameter, it is basically a way to makes E&amp;M fields to travel slightly faster than the speed of light. For relativistic flows, you can set Corr=1.025. ntimes is the number of times to apply a 2D Gaussian smoothing filter to the current before depositing the current on the grid. It helps with high frequency fluctuations which can be caused by noise, numerical Cherenkov, or electrostatic fluctions from too low ppc0 and/or resolution. Set ntimes to 4. Restart . &lt;restart&gt; irestart = 0 # 1 to restart the simulation from saved # restart/*.d files. intrestart = 10000 # How often to save restart files. # They overwrite previous *d files. laprestart = 0 # if different from 0, restart from a # named restart file, # saved at timestep laprestart namedrestartint = 80000000 # interval for saving named restart files, # like restart.lap01234.d . This part of the file is for restarting simulations after they have ended. If you want to restart a simulation, you need to resubmit the job from the same directory as before but with irestart equal to 1. For now, let’s not do anything. Output . &lt;output&gt; intervaal = 500 # plot interval pltstart = 0 # starting iteration for first plot istep = 2 # downsampling factor for grid output stride = 20 # particle stride for particle output ############################################################### writetestlec = 0 # write test electrons for tracking dlaplec = 90 # interval teststartlec = 1000000 # starting lap testendlec = 6000000 writetestion = 0 # write test ions for tracking dlapion = 600 # interval teststartion = 1000000 # starting lap testendion = 6000000 ############################################################### . Here we can change how often we output files. interval is how often the data will be saved. ptlstart is the first time to start saving. When saving fields data, it is downsampled by a factor istep in all directions. Similarly the particle arrays are downsampled by a factor stride. The stuff inside of the # is related to particle tracking, don’t worry about it for now. Boundaries . &lt;boundaries&gt; periodicx = 0 # periodic boundaries in the x direction? periodicy = 1 # periodic boundaries in the y direction? periodicz = 1 # periodic boundaries in the z direction? . Pretty self-explanatory. Change nothing. Domain . &lt;domain&gt; enlarge = 1 # if 1, enlarge box in the x direction if # injector is close to right wall . Setting enlarge to 1 makes the box expand to the right with time. For now, let’s set it to 0. B-field setup . &lt;fields&gt; btheta = 70 # bfield angle bphi=0 -&gt; bz, # bphi=90 in x-y plane, btheta=0-&gt; parallel bphi = 90 # . We’re going to run an unmagnetized shock so btheta and bphi are meaningless. Particles . &lt;particles&gt; sigma = 0.1 # magnetization number (omega_ce/omega_pe)^2, # including gamma0 for inertia maxptl0 = 1e9 # max number of particles in the simulation ppc0 = 16 # number of particles per cell delgam = 1.e-4 # delta gamma for ions # delgam = k T_i / m_i c^2, # T_i is ion temperature me = 1. # electron mass mi = 100. # ion mass # (actually mass to charge ratio) gamma0 = 0.1 # flow drift gamma. If &lt; 1, # interpreted as v/c. # the drift of background plasma # in the negative x direction c_omp = 10 # electron skin depth in cells . Here we edit some particle &amp; field quantities. sigma is the magnetization and it’s definition shows up in binit in the user file. Let’s run an unmagnetized shock; set sigma to zero. maxptl0 is the total number of particles the simulation can handle, for very long runs, you may need to increase it. Keep it at 1 billion for now. ppc0 is the total number of particles in each initialized cell. ppc0=16 means there are 8 electrons and 8 ions. delgam is the temperature of the ions, and the mass of the ions is controlled via me and mi. In theory one could change both of these quantities, but you should generally just change mi. Let’s run a pair shock, so set mi to 1. gamma0 is the flow speed in the leftward direction. Set gamma0 to 0.5, i.e., half the speed of light. Problem . &lt;problem&gt; distr_dim = 3 # Dimensionality of particle distirbution; # 2 means Tx=Ty=T, Tz=0; 3 means Tx=Ty=Tz=T # if distr_dim = 2 and sigma is non-zero, # code will reset it to 3. temperature_ratio = 1 # T_e/T_i external_fields = 0 # if nonzero, add external nonevolving fields # to mover; defined in user file sigma_ext = 0. # strength of external magnetization, # (omega_ce/omega_pe)^2, # including gamma0 user_part_bcs=1 # call particle_bc_user routine from user file, # specify particle bcs like walls wall = 1 # left wall on? wallgam = 0 # gamma of the moving reflecting wall on the left. # If &lt; 1, read as v/c. Set to 0 or 1 to # not move the wall. left_wall_speedup = 1. # remove downstream a little faster; # used for moving left wall, 1 is no speedup. betainj = 0.2 # how fast injector is receding betainj_interval = 2000 # how often to check file adjust_params where betainj # can be modified as the code runs rightclean = 1 # jumping right injector, on average moving at betainj rightwall = 1 # reflecting particle wall at injector . These are the parts of the input file that are defined in the user file. I don’t want to explain every parameter, their definitions can be deduced by reading the source code of your user file. Let’s just focus on the ones you will change. left_wall_speedup periodically removes parts of the downstream of the shock. Set to zero. beta_inj is the average speed of the expanding box. I am not sure if it matters when enlarge is set to zero, but set to zero anyway. righclean turns on the jumping injector. The injectors moves faster than beta_inj on every timestep to the right but jumps backwards periodically to ensure that the average speed is beta_inj. Set to zero. rightwall puts a wall on the right side of the box. For cold flows it doesn’t matter. Set to zero. You are now done editing the parameter file. Close and save your nano session using ctrl+x and typing y and enter. ",
    "url": "http://localhost:4000/GettingStarted/Running-your-first-Tristan-MP-simulation/#editing-the-input-file",
    "relUrl": "/GettingStarted/Running-your-first-Tristan-MP-simulation/#editing-the-input-file"
  },"30": {
    "doc": "Running Tristan-MP",
    "title": "Submitting jobs",
    "content": "To run a big job on a cluster, you cannot simply set it to run. Instead you send your job to a scheduler using a submit script. The scheduler then adds your job to a queue. The scheduler, not you, starts running your code. The specifics of the scheduler depends on the cluster, and you should always read the recommended submit file for the machine you are using. Perseus uses SLURM. Slurm needs a submit shell script. The script tells slurm how many resources you need and what job you want run. We’ll create a submit file now. By opening a new empty text file called submit in nano . nano submit . The first line of your submit file just tells the schedule you want to interpret it using bash. Add this to your file . #!/bin/bash . Next, we need to tell slurm how many resources we will need. In the input file we set sizex=4, sizey=7, meaning we will start 4x7=28 MPI jobs or as slurm calls them ‘tasks’. Each cpu will have its own task, the default for slurm. You request tasks with the --ntasks= flag (or -n). Add a line to your submit script so slurm knows you need 28 tasks. #SBATCH --ntasks=28 . It is nice to also specify the number of nodes, although technically not always 100% necessary. Each node has 28 cores, and you most likely will want to fully fill nodes. You request nodes with the --nodes= flag (or -N). Add a line to make slurm ask for 1 node . #SBATCH --nodes=1 . You should add some lines that makes slurm email you when your job starts and stops. Add the following lines, and change it to your personal email. Slurm may start are stop your job at unexpected times. If it is important to know when your job starts, you may want to choose an email you that will send you notifications. #SBATCH --mail-type=begin #SBATCH --mail-type=end #SBATCH --mail-user=YOUR_EMAIL@THEINTER.NET . Finally we need to tell slurm how long you want your job to run for using the --time flag (or -t). If your job ends before the time is up or crashes, slurm deallocates the resources. Let’s run for 1 hour. Acceptable time formats include ‘minutes’, ‘minutes:seconds’, ‘hours:minutes:seconds’, ‘days-hours’, ‘days-hours:minutes’ and ‘days-hours:minutes:seconds’. The maximum time you can run a job on perseus is 2 days. #SBATCH --time=01:00:00 . Now we have given slurm all of our options, and we are ready for it to run the code. In general, you’ll need to make sure that your code will run after you have loaded the modules that you compiled tristan-mp with to run (i.e., intel, intel-mpi and hdf5). Add the following lines to your submit script. module load intel module load intel-mpi module load hdf5/intel-16.0/intel-mpi/1.8.16 . Now we are ready to run tristan in our submit script. Add the line . srun ./tristan-mp2d -i input.myshock &gt; out . And submit the job by running . sbatch submit . Slurm will respond with Submitted batch job &lt;XXXXXXX&gt;, where IGNORE REMAINING PART . ",
    "url": "http://localhost:4000/GettingStarted/Running-your-first-Tristan-MP-simulation/#submitting-jobs",
    "relUrl": "/GettingStarted/Running-your-first-Tristan-MP-simulation/#submitting-jobs"
  },"31": {
    "doc": "Running Tristan-MP",
    "title": "Checking in on running jobs",
    "content": "nano -w submit (enables you to modify the submit file) . Make sure that -i input.shock is included in the last line of the submit file, right before the ` &gt; out`. After completing the parameter settings, type: . sbatch submit . Type showq |grep netID to view the progress. If you want to cancel a job, find the slurm - xxxxx file in the run directory and type: . scancel xxxxx . ",
    "url": "http://localhost:4000/GettingStarted/Running-your-first-Tristan-MP-simulation/#checking-in-on-running-jobs",
    "relUrl": "/GettingStarted/Running-your-first-Tristan-MP-simulation/#checking-in-on-running-jobs"
  },"32": {
    "doc": "Running Tristan-MP",
    "title": "Running Tristan-MP",
    "content": "This section assumes you have already logged into perseus and followed the instructions in Downloading and Compiling Tristan-mp . ",
    "url": "http://localhost:4000/GettingStarted/Running-your-first-Tristan-MP-simulation/",
    "relUrl": "/GettingStarted/Running-your-first-Tristan-MP-simulation/"
  },"33": {
    "doc": "Running Tristan",
    "title": "Running Tristan-mp",
    "content": "To run the code you need first to have the binary files prepared for the system in question. For instructions on how to compile, please refer to the Compilation section. What follows are just general procedures that you will have to do in order to run the code; for more details on how to prepare an input file you should refer to the Input Structure section, or / and to the two examples provided (the Weibel instability, and the Collisionless shock run). ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Running/#running-tristan-mp",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Running/#running-tristan-mp"
  },"34": {
    "doc": "Running Tristan",
    "title": "Preparing a run on your local machine",
    "content": "After having compiled the code you should have two binary files, by default named tristan-mp2d and tristan-mp3d (if you are not interested in 3D runs, you do not have to compile the 3D version). Prepare the input file to run the problem that you want, and then create a directory and copy both the binary and the input file there. The input file must be named “input”. After changing directories to the run directory, just run . mpirun -n &lt;ncpus&gt; &lt;name-of-executable&gt; . (e.g. mpirun -n 2 ./trist-della2d for a 2D run using two cpus). ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Running/#preparing-a-run-on-your-local-machine",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Running/#preparing-a-run-on-your-local-machine"
  },"35": {
    "doc": "Running Tristan",
    "title": "Preparing a run for a regular high-performance cluster",
    "content": "As a first step, please refer to the specific instructions for the cluster where you want to run. These instructions are not to supersede any specific site instructions, but usually running this kind of code in a high-performance cluster goes like this. The first step to run the code is to create a root folder for the run; this folder should be accessible from all the CPU’s (cores) in the run. Usually, there is a specific file system on which to run codes. You should then create a folder there using mkdir name-of-the-folder. The next step is to copy the pre-prepared input file for the specific run into the root folder. This input file must be named input. Next, copy the correct version of the binary file also to the root folder you created cp path-to-binary/tristan-mp2d path-to-run-folder/. Finally, create a submit file, which should be used to submit the job to the batch-system (usually PBS or MOAB), which will look something like this: . #!/bin/bash #PBS -q regular #PBS -m abe #PBS -j oe #PBS -l nodes=8:ppn=8 #PBS -l walltime=12:00:00 cd $PBS_O_WORKDIR export PBS_JOBNAME=\"my1024\" echo $PBS_JOBNAME &gt; jobname echo $PBS_JOBID &gt; jobid cat $PBS_NODEFILE &gt; hosts.$PBS_JOBID mpiexec --mca btl ^tcp ./tristan-mp2d &gt; out . ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Running/#preparing-a-run-for-a-regular-high-performance-cluster",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Running/#preparing-a-run-for-a-regular-high-performance-cluster"
  },"36": {
    "doc": "Running Tristan",
    "title": "System dependent tasks",
    "content": "For completeness it should now be stated that the final step to run the code is probably to submit the run script (as the one above) to the correct queue. This is usually done in most clusters by issuing the command qsub &lt;submit-filename&gt; (for torque-type queuing systems). The specific command might vary, and the contents required to be in the run script file will most certainly vary as well. To determine what to do for each specific system you must consult the cluster user documentation / webpage. ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Running/#system-dependent-tasks",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Running/#system-dependent-tasks"
  },"37": {
    "doc": "Running Tristan",
    "title": "Code output",
    "content": "The code saves log messages in the file “out”, sequentially numbered output files in a subdirectory “output”, and restart files in a subdirectory “restart”. The code overwrites present files, and creates subdirectories if they don’t exist. ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Running/#code-output",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Running/#code-output"
  },"38": {
    "doc": "Running Tristan",
    "title": "Running Tristan",
    "content": "This page is deprecated from previous instructions on tristan-MP. Please see this instead. ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/Running/",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/Running/"
  },"39": {
    "doc": "Weibel Simulation",
    "title": "Weibel Simulation",
    "content": "Below is an example of a periodic box with counter-moving cold beam, which is unstable to the Weibel instability. You can reproduce these results similar to these by following the instructions here and here but compiling with the command . make USER_FILE=user/user_weibel . When submitting the job, use the input file located at ~/user/input.weibel and run it using: . srun ./tristan-mp2d -i input.weibel &gt; out . ",
    "url": "http://localhost:4000/Example%20Simulations/Sample-Weibel-run/",
    "relUrl": "/Example%20Simulations/Sample-Weibel-run/"
  },"40": {
    "doc": "Shock Simulation",
    "title": "Shock Simulation",
    "content": "Below is an example of an unmagnetized shock, formed via the Weibel instability. You can reproduce these results similar to these by following the instructions here. ",
    "url": "http://localhost:4000/Example%20Simulations/Sample-shock-run/",
    "relUrl": "/Example%20Simulations/Sample-shock-run/"
  },"41": {
    "doc": "User Files",
    "title": "User file",
    "content": "The user file, user_*.F90, such as user_shock.F90 or user_weibel.F90, is used for readily expanding the behavior of the code without having to alter any of the code-flow logic. The user defines routines for loading and injecting particles here, as well special boundary conditions on fields and particles (e.g. reflecting walls) and external fields. subroutine get_external_fields . This subroutine compute external magnetic fields to be added to the particle mover. These fields do not evolve via Maxwell equations. Note that it uses local coordinates corresponding to different CPU, so one should convert that to global coordinates first. Example: This example sets an external B_z field, that is an bz_ext0*sin(x)*sin(y) function of global coordinate x and y. subroutine get_external_fields real,intent(inout):: bx_ext,by_ext,bz_ext, ex_ext, ey_ext, ez_ext real, intent(in):: x,y,z ex_ext=0. ey_ext=0. ez_ext=0. bx_ext=0. by_ext=0. bz_ext=bz_ext0*sin(x)*sin(y+modulo(rank,sizey)*(myall-5)) !bz_0 is defined in the input reading part above, !note the coordinate transformation rule. end subroutine get_external_fields . subroutine init_EMfields_user() . This subroutine sets the electromagnetic fields of any specific user purpose. Example: This example sets an initial jump for in-plane magnetic field by. Also component perpendicular to interface surface bx is added. subroutine init_EMfields_user() integer :: i, j, k, jglob, kglob !initialize B field to be set by Binit and the inclination angle !-- used for shock problems Binit=sqrt((gamma0)*ppc0*.5*c2*(mi+me)*sigma) do k=1,mz do j=1,my do i=1,mx jglob=j+modulo(rank,sizey)*(myall-5) !global j,k coords are defined from local ones kglob=k+(rank/sizey)*(mzall-5) !magnetic fields components are initialized bx(i,j,k)=0.5*Binit bz(i,j,k)=0.*Binit*sin(btheta)*sin(bphi) by(i,j,k)=2.*Binit*tanh((i-mx/2)/4.) ex(i,j,k)=0. !electric field is initialized ey(i,j,k)=0. ez(i,j,k)=-0. enddo enddo enddo end subroutine init_EMfields_user . ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/User-Files/#user-file",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/User-Files/#user-file"
  },"42": {
    "doc": "User Files",
    "title": "User Files",
    "content": " ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/User-Files/",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/User-Files/"
  },"43": {
    "doc": "User Manual",
    "title": "User’s Manual",
    "content": "Tristan-mp stands for TRIdimensional STANford - Massively Parallel code, and is the parallel version [1] of the code originally developed by O. Buneman, K. Nishikawa, and T. Neubert [2]. In its current form, the code is written in a modular format in Fortran 95, and uses the MPI (e.g. see Open MPI) and HDF5 libraries to support parallelism and standardized parallel output files. It is a fully relativistic Particle-In-Cell (PIC) code used for plasma physics computations; it self-consistently solves the full set of Maxwell’s equations, along with the relativistic equations of motion for the charged particles. It follows the general PIC code architecture [3,4]: fields are discretized on a finite 3D or 2D mesh, the computational grid, and this field is then used to advance the velocity of the particles in time via the Lorentz force equation. The charges and currents derived from the particles’ velocities and positions are then used as source terms to re-calculate the electromagnetic fields. The PIC simulation model is described below, along with the details of the numerical implementation of the physical equations. Particle-In-Cell simulation model The basic set of equations solved by Tristan-mp are Maxwell’s equations: . (1) . (2) . along with the Lorentz force equation, and the relativistic dynamic equations for the electrons and ions . (3) . (4) . The particles’ positions and velocities are used to calculate the current, which is used as a source term in Eq. (2). To self-consistently solve this set of equations, the electromagnetic fields and particles are first initialized in the simulation box, assuring initially divergence-free Electric and Magnetic fields, which is accomplished by initializing electrons and ions (or positrons) in the same spatial positions. The finite difference numerical implementation of the above equations, which usually varies from code to code, along with the numerical methods used, are presented in the next section. Numerical Implementation . Tristan-mp uses time-centered and space-centered finite difference schemes to advance the equations in time, and to calculate spatial derivatives, so that the algorithm is second order accurate in space and time. A 3D Yee mesh [5] is used to store the Magnetic and Electric fields (see Fig. 1), and a tri-linear interpolation function (linear in each spatial dimension) is used to interpolate the Electric and Magnetic fields to the particles’ positions. Furthermore, a three point digital binomial filter [3,4], with weights 0.25, 0.5, 0.25, is used along each spatial dimension on the source terms for the field equations, in order to suppress non-physical high-frequency field modes, which are due to the finite difference nature of the derivative calculations. Also, there is an option to select from the regular second order finite difference method [5], and a fourth order stencil as set forward in [6]; the purpose of the fourth order stencil is to reduce the effect of numerical Cerenkov instability, which might become a relevant factor for some relativistic runs [6]. (4) . The electromagnetic fields are used to advance the particles’ velocities using Eq. (3). There are two particle movers available in the code, both solving Eq. (3) and Eq. (4) explicitly: an implementation of the Boris algorithm [6], and an implementation of the Vay pusher [7]. These are two similar methods, with the Vay pusher having some advantages over the regular Boris algorithm in the relativistic regime. The main PIC loop in Tristan-mp proceeds as follows, given the initial Electric field, Magnetic field, and Particles’ positions and velocities: . | Advance the Magnetic field by half a time step . | Advance particles velocities and positions in time (using the Boris or Vay algorithms, Boris depicted here), and collect the current . | Advance the Magnetic field by another half time step . | Advance the electric field the full time step . | . Simulation and physical units . Tristan employs an unconventional system of electromagnetic units. Details about this system of units can be found in this document (it has several typos, unfortunately). Note that, in practice, detailed understanding of this system of units is not necessary, as most every quantity can be expressed in dimensionless units. E.g. time normalized by plasma frequency, space by skin depths, speeds in terms of speed of light. By constructing dimensionless ratios, it is possible to avoid ever needing the conversion of any quantity into cgs. Useful facts about code units: Electron plasma frequency in code units (inverse timesteps) is omega_pe = sqrt( q_e n / ((m/m_e) gamma)), where q is the charge of the particle, n density and gamma is the typical lorenz factor, m is particle’s mass. Note that here 4 pi = 1 compared to cgs, and q_e/m_e = 1 by definition. Cyclotron frequency: omega_c = B/(gamma (m/m_e) c) (in inverse time steps). Magnetization sigma = (omega_c)^2/ (omega_p)^2 (c/v)^2 = 1/M_A^2, where M_A is the alfvenic Mach #. ",
    "url": "http://localhost:4000/User%20Manual/User-Manual/#users-manual",
    "relUrl": "/User%20Manual/User-Manual/#users-manual"
  },"44": {
    "doc": "User Manual",
    "title": "Bibliography",
    "content": ". | Spitkovsky, A., Simulations of relativistic collisionless shocks: shock structure and particle acceleration, in AIP Conf. Ser. 801, ed. T. Bulik, B. Rudak, &amp; G. Madejski (Melville, NY: AIP), 345, 2005 | Buneman, O., Computer Space Plasma Physics (Terra Scientific, Tokyo), 67, 1993 | Birdsall, C. K., and Langdon, B., Plasma Physics via Computer Simulation (McGraw-Hill, New York), 1985 | Hockney, R. W., and Eastwood J. W., Computer Simulation using Particles (McGraw-Hill, New York), 1981 | Yee, K. S., Numerical Solution of Initial Boundary Value Problems Involving Maxwell’s Equations in Isotropic Media, IEE Trans. on Ant. and Prop., 14, 3, 202, 1966 | Greenwood, A. D., Cartwright, K. L., Luginsland, J. W., and Baca E. A., On the elimination of numerical Cerenkov radiation in PIC simulations, JCP 201, 665, 2004 | Boris, J. P., Relativistic plasma simulation-optimization of a hybrid code, In Fourth Conf. Num. Sim. Plasmas, page 3, Washington DC, Naval Res. Lab, 1970 | Vay, J.-L., Simulation of beams or plasmas crossing at relativistic velocity, Phys. of Plasmas 15, 056701, 2008 | . ",
    "url": "http://localhost:4000/User%20Manual/User-Manual/#bibliography",
    "relUrl": "/User%20Manual/User-Manual/#bibliography"
  },"45": {
    "doc": "User Manual",
    "title": "User Manual",
    "content": "NEEDS UPDATING! . ",
    "url": "http://localhost:4000/User%20Manual/User-Manual/",
    "relUrl": "/User%20Manual/User-Manual/"
  },"46": {
    "doc": "Writing a python analysis script",
    "title": "Below is an example script to do some rudimentary analysis of tristan-MP outputs.",
    "content": "The first part of the script we will load numpy, the default python library for numerical work, matplotlib, the default python 2D plotting library, and h5py, the library that allows us to access our data output files. We’ll also load the os library which will let us interact with our directories etc. import numpy as np import matplotlib.pyplot as plt import h5py, os . First let’s have a variable for our output directory . outdir = '/PATH/TO/YOUR/output/' . Tristan saves 4 files every output, param.***, flds.tot.***, prtl.tot.*** and spect.***, for each output timestep; e.g. param.001, param.002, … It’s useful to look at our output directory and make sure all the timesteps that has all four of these files in our outdir . First we will make a list of all of the files in the output directory that start with ‘param’, ‘flds’, ‘prtl’ or ‘spect’ . fnum = filter( lambda x: x.split('.')[0] in ['param', 'flds', 'prtl', 'spect'], os.listdir(outdir) ) . fnum is an iterable that has the values= [‘param.001’, ‘flds.001’,… ,’spect.NNN’] We only need the last bit of the file that is the output number . fnum = list(map(lambda x: x.split('.')[-1], fnum)) . Fnum is a list that looks like [‘001’, ‘001’, … , ‘NNN’, ‘NNN’]. We will want the outputs that have all 4 output files. Convert fnum into a set so that only unique values are saved. Then, only keep values that have 4 elements in fnum . fnum = sorted(filter(lambda x: fnum.count(x) == 4, set(fnum))) . fnum is a list (technically an iterable) that contains all of the ‘***’ endings to the outfiles, but only 1 of each ([‘001’, ‘002’, …]). Soon we’ll iterate over all the files to plot Bsq vs time, but beforehand let’s see what values each outfiles have saved. We can see the keys available to us by loading the file with h5py . for elm in ['param.', 'flds.tot.', 'prtl.tot.', 'spect.']: with h5py.File(outdir + elm + fnum[0], 'r') as f: print('The quantities available in the ' + elm + 'files are...') print([key for key in f.keys()]) . See tristan-mp output.F90 file, or ask a friend to figure out what each of these keys are. Let’s say we want to plot the total B-field as a function of time. First make a list to hold the Btot values and time values. Btot = [] t = [] . Iterate over the output numbers. for num in fnum: # For each output, we have to load the param file to get the time with h5py.File(outdir + 'param.' + num, 'r') as f: # Append it to our list t.append(f['time'][0]) # Now, load the fields file and get the B value with h5py.File(outdir + 'flds.tot.' + num, 'r') as f: # Let's assume the simulation is 2D # The simulation saves Bx, By, Bz separately # so we must square them, take square root, then # sum over the box. Btot.append( np.sum( np.sqrt( f['bx'][0, :, :]**2 + f['by'][0, :, :]**2 + f['bz'][0, :, :]**2 ) ) ) . We can plot our list using matplotlib . plt.plot(t, Btot) # Add labels to the plot plt.xlabel(r'$\\omega_{pe}t$') plt.ylabel(r'$B^2 \\ [{\\rm arb. unit}]$') plt.title('Total Magnetic energy vs time') plt.show() . Here’s the script is in its entirety: . import numpy as np import matplotlib.pyplot as plt import h5py, os # First let's have a variable for our output directory outdir = '/PATH/TO/YOUR/output/' # Tristan saves 4 files every output, `param.***`, `flds.tot.***`, # `prtl.tot.***` and `spect.***`, for each output timestep; # e.g. param.001 param.002, ... It's useful to look at our output # directory and make sure all the timesteps that has all four of these # files in our outdir # First we will make a list of all of the files in the output directory that # start with 'param', 'flds', 'prtl' or 'spect' fnum = filter( lambda x: x.split('.')[0] in ['param', 'flds', 'prtl', 'spect'], os.listdir(outdir)) # We only need the last bit of the file that is the output number fnum = list(map(lambda x: x.split('.')[-1], fnum)) # We will want the outputs that have all 4 output files. Convert # fnum into a set so that only unique values are saved. Then, only # keep values that have 4 elements in fnum fnum = sorted(filter(lambda x: fnum.count(x) == 4, set(fnum))) # Now fnum is a list (technically an iterable) that contains all of # the '***' endings to the outfiles. Soon we'll iterate over all the # files to plot Bsq vs time, but beforehand let's see what values each # outfiles have saved. # We can see the keys available to us by loading the file with h5py for elm in ['param.', 'flds.tot.', 'prtl.tot.', 'spect.']: with h5py.File(outdir + elm + fnum[0], 'r') as f: print('The quantities available in the ' + elm + 'files are...') print([key for key in f.keys()]) # See tristan-mp output.F90 file, or ask a friend to figure out what # each of these keys are. # Let's say we want to plot the total B-field as a function of time. # First make a list to hold the Btot values and time values. Btot = [] t = [] # Iterate over the output numbers. for num in fnum: # For each output, we have to load the param file to get the time with h5py.File(outdir + 'param.' + num, 'r') as f: # Append it to our list t.append(f['time'][0]) # Now, load the fields file and get the B value with h5py.File(outdir + 'flds.tot.' + num, 'r') as f: # Let's assume the simulation is 2D # The simulation saves Bx, By, Bz separately # so we must square them, take square root, then # sum over the box. Btot.append( np.sum( np.sqrt( f['bx'][0, :, :]**2 + f['by'][0, :, :]**2 + f['bz'][0, :, :]**2 ) ) ) # Now we can plot our list using matplotlib plt.plot(t, Btot) # Now we should add labels to the plot plt.xlabel(r'$\\omega_{pe}t$') plt.ylabel(r'$B^2 \\ [{\\rm arb. unit}]$') plt.title('Total Magnetic energy vs time') plt.show() . ",
    "url": "http://localhost:4000/AnalyzingSims/Writing-your-own-python-scripts/#below-is-an-example-script-to-do-some-rudimentary-analysis-of-tristan-mp-outputs",
    "relUrl": "/AnalyzingSims/Writing-your-own-python-scripts/#below-is-an-example-script-to-do-some-rudimentary-analysis-of-tristan-mp-outputs"
  },"47": {
    "doc": "Writing a python analysis script",
    "title": "Writing a python analysis script",
    "content": " ",
    "url": "http://localhost:4000/AnalyzingSims/Writing-your-own-python-scripts/",
    "relUrl": "/AnalyzingSims/Writing-your-own-python-scripts/"
  },"48": {
    "doc": "Home",
    "title": "tristan-mp-pitp",
    "content": "TRISTAN-MP parallel electromagnetic 3D particle-in-cell code. Pre-release version for testing. Developed by: Anatoly Spitkovsky, Luis Gargate, Jaehong Park, Lorenzo Sironi. Based on original TRISTAN code by Oscar Buneman. See (Home.md) for more extensive documentation. Warning: some of the info on the wiki is obsolete. Organization and suggested workflow: The code is modular and is organized in such a way that in most cases the user would not need to edit any of the main source files in the main tristan-mp directory. All of the user-specific configuration should be possible to confine to user_* routines. There are three example user configuration files in main directory, showing a counterstreaming Weibel instability setup, two-stream instability and a collisionless shock simulation. There are also sample input files. When using code from github, clone it to your local machine, switch to “master” branch. You can create your branch off of master. On local machine: git clone https://github.com/ntoles/tristan-mp-pitp.git . copy some example files to start with . cp user_weibel.F90 user_mysetup.F90 . cp ../Makefile Makefile.mysetup . edit user_mysetup.F90 edit Makefile.mysetup to add USER_FILE=user_mysetup (no need for extension F90) . To compile . cd source directory make -f Makefile.mysetup clean make -f Makefile.mysetup . 3D version is enabled when -DtwoD flag is omitted from the Makefile. You need to have parallel HDF5 library installed with intel or GNU compilers, which will create h5pfc alias for the compiler. Some instructions for installation are on wiki page. For Macs brew seems to work fine with gfortran: . $ brew install gcc $ brew install openmpi --enable-mpi-thread-multiple $ brew install hdf5 --with-fortran --with-mpi . This will produce tristan-mp2d executable. To run: Make a run directory somewhere outside the git-controlled source directory. Copy the executable tristan-mp2d there. Copy example submit and input files from directory (see wiki page for example submit files for clusters; you don’t need these on your desktop/laptop). Input file has to be named “input” in the run directory, or the executable takes -i option. E.g.: ./tristan-mp2d -i input.weibel (for MPI, it can be, e.g.: srun -n 16 ./tristan-mp2d -i input.weibel) Note that you need to edit the input file to set the number of domain sub-partitions sizex * sizey * sizez be equal to the total number of cores to be used. sizez = 1 in 2D. Edit submit and input files for your case and according to the queue policy of your cluster. In 3D, the domain is split in y and z directions, with “sizey” chunks in y direction and “total # of cpus/sizey” chunks in z direction. qsub submit or other appropriate submission command. When running, the code will create subdirectories output and restart. The output is single HDF5 files (single per time step). Currently we provide routines to interactively view output using python. https://github.com/pcrumley/Iseult . It requires anaconda to run. We had good experience with anaconda 4.0.0 on Mac, but not later. The older version is available on anaconda’s website. To launch the vis tool, run (path to Iseult)Iseult/iseult.py . This will open interactive windows. Right click on plots to get more options. load.py is a script that loads HDF5 files into a python dictionary, that can be accessed as d[i][‘bz’], where i is the file number. There are also older IDL routines, which are available on request. Good luck! . ",
    "url": "http://localhost:4000/#tristan-mp-pitp",
    "relUrl": "/#tristan-mp-pitp"
  },"49": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"50": {
    "doc": "Nersc Cori",
    "title": "Logging into Cori Nodes",
    "content": "First you need to get a NIM account and password with MFA. Once you have that set up, ssh into cori. ssh user@cori.nersc.gov . All of your simulations should be run from your $SCRATCH directory, so cd there. IMPORTANT NERSC will delete files left in the $SCRATCH directory every ~2 weeks or so (check official policy if you need to) . cd $SCRATCH . For the most part, running tristan-mp on Cori is simple, except Cori no longer supports the older version of HDF5 that tristan-mp requires, so you’ll need to compile your own. Here are the instructions to compile tristan-mp with hdf5-1.8 on Cori. Download the hdf5-1.8.21.tar to your $SCRATCH folder: located here and unzip it. tar -xvf hdf5-1.8.21.tar . You’ll need to compile and install HDF5. The easiest way to do that is create a file called install.sh in the unzipped HDF5 directory. The file should contain the following . #!/bin/bash H5_HOME=/PATH/TO/hdf5-1.8.21/ # modify this to your path to hdf5 ./configure --prefix=$H5_HOME --enable-fortran --enable-parallel \\ --enable-java --enable-shared CFLAGS=\"-fPIC -Ofast\" \\ FCFLAGS=\"-fPIC -Ofast\" LDFLAGS=\"-dynamic\" FC=ftn CC=cc \\ CXX=CC --enable-build-mode=production --enable-unsupported make -j 4 # parallel compile make install # install . Finally you’ll compile tristan following the instructions here, except you will need to change your tristan-mp Makefile to point to version of HDF5 you just compiled. You can keep all parts the same except change the following flags and variables where they occur in the original Makefile . cc = icc FC = /PATH/TO/hdf5-1.8.21/bin/h5pfc # replace with the actual path LD = /PATH/TO/hdf5-1.8.21/bin/h5pfc PERFORMANCE = -O3 -I/PATH/TO/hdf5-1.8.21/include -L/PATH/TO/hdf5-1.8.21/lib . ",
    "url": "http://localhost:4000/RunningOnOtherClusters/nersc/#logging-into-cori-nodes",
    "relUrl": "/RunningOnOtherClusters/nersc/#logging-into-cori-nodes"
  },"51": {
    "doc": "Nersc Cori",
    "title": "Nersc Cori",
    "content": " ",
    "url": "http://localhost:4000/RunningOnOtherClusters/nersc/",
    "relUrl": "/RunningOnOtherClusters/nersc/"
  },"52": {
    "doc": "Tristan-MP Features and Code Structure",
    "title": "Tristan-MP Features and Code Structure",
    "content": ". | Overview . | General Code Structure . | Main Files | Main Algorithms | Outputs | Initialization | Communication | Auxiliary Modules | User Modules | . | Input Files . | Input File Structure | Sample Input File | . | User Files . | . | . ",
    "url": "http://localhost:4000/Tristan-MP%20Features%20And%20Code%20Structure/outline/",
    "relUrl": "/Tristan-MP%20Features%20And%20Code%20Structure/outline/"
  },"53": {
    "doc": "Analyzing Simulations",
    "title": "Analyzing and Visualizing your simulations",
    "content": ". | Tigressdata2 . | Starting a VNC server | Connecting to Tigressdata2 | Closing a VNC Session | Enabling Tigressdata2’s GPUs | Accessing Jupyter Notebook using ssh | . | Writing your own python analysis scripts . | . ",
    "url": "http://localhost:4000/AnalyzingSims/outline/#analyzing-and-visualizing-your-simulations",
    "relUrl": "/AnalyzingSims/outline/#analyzing-and-visualizing-your-simulations"
  },"54": {
    "doc": "Analyzing Simulations",
    "title": "Analyzing Simulations",
    "content": " ",
    "url": "http://localhost:4000/AnalyzingSims/outline/",
    "relUrl": "/AnalyzingSims/outline/"
  },"55": {
    "doc": "Getting Started",
    "title": "Getting started",
    "content": " ",
    "url": "http://localhost:4000/GettingStarted/outline/#getting-started",
    "relUrl": "/GettingStarted/outline/#getting-started"
  },"56": {
    "doc": "Getting Started",
    "title": "Setting up your personal computer to access Princeton’s cluster",
    "content": " ",
    "url": "http://localhost:4000/GettingStarted/outline/#setting-up-your-personal-computer-to-access-princetons-cluster",
    "relUrl": "/GettingStarted/outline/#setting-up-your-personal-computer-to-access-princetons-cluster"
  },"57": {
    "doc": "Getting Started",
    "title": "Running your first Tristan-MP Job on Princeton’s cluster",
    "content": ". | Using Perseus . | Logging into Perseus and configuring your environment | Familiarizing yourself with the Filesystem | . | Downloading and Compiling Tristan-MP . | Downloading Tristan-MP from github | Starting your own branch | Compiling Tristan-MP | . | Running your first Tristan-MP Simulation . | Getting ready | Editing the input file | Submitting the job | . | . ",
    "url": "http://localhost:4000/GettingStarted/outline/#running-your-first-tristan-mp-job-on-princetons-cluster",
    "relUrl": "/GettingStarted/outline/#running-your-first-tristan-mp-job-on-princetons-cluster"
  },"58": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "Welcome to the tristan-mp-pu documentation! . ",
    "url": "http://localhost:4000/GettingStarted/outline/",
    "relUrl": "/GettingStarted/outline/"
  },"59": {
    "doc": "Running On Other HPC Clusters",
    "title": "Running On Other HPC Clusters",
    "content": " ",
    "url": "http://localhost:4000/RunningOnOtherClusters/outline/",
    "relUrl": "/RunningOnOtherClusters/outline/"
  },"60": {
    "doc": "Example Simulations",
    "title": "Example Simulations",
    "content": " ",
    "url": "http://localhost:4000/Example%20Simulations/outline/",
    "relUrl": "/Example%20Simulations/outline/"
  }
}
